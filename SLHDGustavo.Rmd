---
title: "SLHD.Assessement-DSTI"
author: "Gustavo Chinchayan"
date: "8/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Exercise 1: general questions
### 1. 
A resampling method can be used for parameter tuning. This is a tool consisting of repeatedly drawing samples from a training data set and calculating statistics and metrics on each of those samples to obtain further information about the performance of a model. One of the methods of effective resampling technique is known as V-fold cross validation (CV). The V-fold cross validation is a robust method for estimating the accuracy of a model.
The V-fold CV method starts by:

* Randomly splitting the data set into V number of subsets (5 or 10).
* One subset is reserved, and all the other subsets are used to train the model.
* Test the model on the reserved subset and record the prediction error
* Repeat this process until each of the V subsets has served as the test set.
* Compute the average of the V recorded error (also called the cross-validation error) used as the performance metric. 


![sample of cross-validation](C:/Users/USER/Documents/R/R Exercize/Cross Validation.jpg)

Other Resampling methods are:

Leave one out Method: Which each observation is considered as the validation set and the rest (N-1) observations are considered as the training set.This is a special case of V-fold CV, its known to reduce bias and randomness.

BootStrapping: Independently sampling with replacement from an existing sample data with same sample size and performing inference among these resampled data. Examples of statistical inferences can be estimating standard error and obtaining confidence interval.


### 2. 
A Gaussian mixture model is a model-based clustering where each data point has a probability of belonging to each cluster. In combination with the EM algorithm for inference, it is used to find the local maximum likelihood.  The Gaussian mixture model comes with a family of different sub-models defined by their constraints of the covariance (spherical, diagonal, tied, and full). One such technique to find the number of clusters is known as the Bayesian Information Criterion (BIC). The BIC takes the log. likelihood of the model and adds a penalty which is dependent on the complexity of the model. It is the count of the scalar parameters for each sub models with constraints. In practice, several sub-models are compared at the same time with different number of K (number of groups) resulting in different BIC, therefore the model with the highest value of BIC is selected.

# Exercise 2: hierchical clustering
### 1. 
A Criterion to measure the clustering quality (Determines how well each object lies within its cluster) is the following:

J(K) = 	$\frac{B(maximize)}{W(minimize)}$ in practice we do $\frac{B}{S}$, which both are equivalent (same optimization)
In Clustering context:

A) $S$ is the general variance (this is fixed)
B) $B$ is the between group variance
C) $W$ is the within group variance

Data Scientists should choose the amount of K based on their understanding of the problem. 


### 2. 
```{r}

h.cl <- matrix(c(0, 1, 0, 2, 1, 1, 3, 1, 3.5, 1, 1, 5, 3, 4, 4, 5), nrow = 8, byrow = TRUE)
```

in $R^{2}$ with single linkage to calculate the distance between two points. 

Calculating distance following this formula:
$$\sqrt{(q_1-p_1)^2 + (q_2-p_2)^2 +.... (q_n-p_n)^2}$$


![Math Data points](C:/Users/USER/Documents/R/R Exercize/slhd assessment _1.jpg)
Math Data points attached for reference 


```{r}
library(factoextra)
h.clust <- dist(h.cl, method = "euclidean")
singl.hc <- hclust(d = h.clust, method = "single")
fviz_dend(singl.hc, cex = 0.5, xlab = "Distance using Single Linkage")
```

Dendrogram generated by fviz_dend() function in factorextra package

# Exercise 3: the Velib Data

### 3.1 Loading the Data
```{r}
load("C:/Users/USER/Documents/R/R Exercize/velib.Rdata")
```

### 3.2 Pretreatment et descriptive analysis
Setting up the dataset

```{r}
X <- velib$data
colnames(X) <- velib$dates
rownames(X) <- paste(c(1:NROW(velib$names)),velib$names)
X$means <- rowMeans(X)


```

#### Pretreatment

Checking the completeness of Dataframe and verifying the class,structure, and dimensions

```{r}
class(X)
dim(X)
```

Checking whether null values exist in this DF

```{r}
is.null(X)
```

#### Descriptive Analysis

Descriptive Statistics
```{r}
summary(X)
```
Summary shows the daily velib bike station occupancy with range, quartiles, median and mean to get a better idea of the distribution of variables in the dataset

Boxplot
```{r}
boxplot(X)
```

As we can see, there is a clear frequency of bike station occupancy on a through out the day and night.
Seems like customers of velib take out bikes from the bikestations at specific time periods

Scatter plot between Dim-11 and Dim-12

```{r}
plot(X[,1:2],pch = 19, col='lavender')
```

Comparing the 2 sundays there is similar correlation when it comes to bike station occupancy rate. 



### 3.3 Data visualization

#### Principal component Analysis
by using princomp to perform PCA

```{r}
pca <- princomp(X)
Yhat = predict(pca)
screeplot(pca)
```

Loading Factoextra Library for comparative purposes to visualize the Screeplot in order to be certain in selecting the right number of dimensions for this test

```{r include=FALSE}
library(factoextra)
```


```{r}
fviz_screeplot(pca, addlabels = TRUE, ylim = c(0, 50))
```

By looking at both Scree tests for comparison purposes, its safe to say that using 2 components seem to capture most amount data


After selecting the number components its best to look at the correlation circle to understand the relationships between the PCA axes and its original variables, we first place them in a biplot. Then we split the two plots in order to achieve clarity.

```{r}
biplot(pca)
```

```{r}
par(mfrow = c(1,2))
biplot(pca,col = c(1,0))
biplot(pca,col = c(0,2)); box()
```

By splitting the biplot datasets, and only focusing on the right hand-side graph that contains the dates (labeled in pink). There seems to be a positive correlation, but with the help of a different library, it can help me explain things better. 

Factoextra Library is again used for comparative purpose agaisnt the earlier biplots


```{r}
fviz_pca_ind(pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE,    # Avoid text overlapping.
             label = FALSE
             )
```

As we can notice in both the Biplot graphs as well as the fviz_pca_ind graph it seems that most of the individual clusters are aligned in the right direction, meaning that there is positive correlation between components 1 and 2.


### 3.4 Clustering

#### 3.4.1 Hierarchical clustering

```{r}
library(leaflet)
velibmap <- velib$position
velibmap$bonus <- velib$bonus

```


```{r}
res.dist <- dist(X, method = "euclidean")
res.hc <- hclust(d = res.dist, method = "complete")
fviz_dend(res.hc, cex = 0.5)

```

Looking at the Hirearchical clustering results with the help of the fviz_dend() function in factorextra package to produce this Dendrogram, I am almost certain that the best number of clusters should be 4 but it can also be 3. The K-means test can be conducted subsequently to finalize this decision. 



```{r}
h.clust <- cutree(res.hc, k = 4)
palette <- colorFactor("RdYlBu", domain = NULL)

```

```{r}
leaflet(velibmap) %>% addProviderTiles(providers$CartoDB.Positron) %>% 
  addCircleMarkers(radius = (X$means)*6,
                   color = palette(h.clust),
                   stroke = ~ifelse(velibmap$bonus == "1", TRUE, FALSE), 
                   label = ~paste(row.names(X), sep = " - Clus.:", ... = h.clust),
                   fillOpacity = 0.9) 
```

#### 3.4.2 k-means


```{r}
Kmax = 10
J = rep(NA,Kmax)
for (k in 1:Kmax){
  out = kmeans(X, centers = k, nstart = 10)
  J[k] = out$betweenss / out$totss
}
plot(J,type='b')
```

Based on the result of this K-means test, it seems certain that 4 clusters it the optimal amount (where the bend occurs also known as the elbow). 
This can further confirm that the right number of clusters is 4 instead of 3 as previously observed in the hierarchical clustering test.

```{r}
K.means <- 4
K.combined <- kmeans(X,centers = K.means, nstart = 10)
```

We can therefore build the same map before by adding the K means cluster number to the previous analysis using leaflet package

```{r}
palette.2 <- colorFactor("RdYlBu", domain = NULL)
leaflet(velibmap) %>% addProviderTiles(providers$CartoDB.Positron) %>% 
  addCircleMarkers(radius = (X$means)*6,
                   color = palette.2(K.combined$cluster),
                   stroke = ~ifelse(velib$bonus == "1", TRUE, FALSE), 
                   label = ~paste(row.names(X), sep = " - Clus.:", K.combined),
                   fillOpacity = 0.9) 

```


### 3.5 Summary

In Summary. we see that both Hierarchical clustering as well as K-means clustering exhibit similar results of 4 clusters when visualizing the data set using the leaflet package.

The 4 clusters allow us to paint the picture that the 2 clusters nearest to the city centre of Paris are closely related, while the other 2 clusters are spread throughout the city and its borders.
There is a few reasons why bike station availability is more in certain areas than the others:

1)On a geographical standpoint, it can be mentioned that bike station availability seems to be more prevalent in zones of higher elevation areas compared to the ones in the city center closer to the Seine River of Paris. Bikes are being used a lot more often in areas of the city center as opposed to the outside areas.
If you look closely at the map provided below, areas where elevation is higher bike station availability is highly concentrated in those clusters. However it cannot explain why bike station availability is present in the southern western region of Paris where elevation is not a factor.

![Paris Elevation Map](C:/Users/USER/Documents/R/R Exercize/elevation paris.jpg)

2) On an income standpoint, another reason as to why bike station availability is prevalent in those areas is because they are in zones of low income compared those in the city centre. If you look at the map below (Could not find a recent map of after 2015), Low median income areas are in the north and north eastern areas of Paris where bikes are highly available. Whereas in city center of Paris bikes are being used a lot more often. Bike station availability is still unexplained in the south western region of Paris


![Paris Median Income 2015](C:/Users/USER/Documents/R/R Exercize/income Paris.jpg)

I think going forward, It would be interesting to know other factors that influence bike station availability in the south western region on Paris. This could possibly be due to other reasons perhaps different preferences in mode of transportation 
